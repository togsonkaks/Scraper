Perfect ‚Äî here‚Äôs how I‚Äôd structure this in Postgres so it‚Äôs clean, traceable, and future-proof:

---

# üîπ Database Schema for Scraped + Enriched Products

---

## 1. `products_raw`

This is the **raw intake bucket** ‚Äî everything scraped goes here first. No fancy processing.

```sql
CREATE TABLE products_raw (
    raw_id SERIAL PRIMARY KEY,
    source_url TEXT NOT NULL,
    raw_title TEXT,
    raw_description TEXT,
    raw_breadcrumbs TEXT[],
    raw_price TEXT,
    raw_images TEXT[],
    scraped_at TIMESTAMP DEFAULT NOW()
);
```

* **Purpose:** Preserve original source.
* **Why:** If scraping logic changes, you can always re-process from here.

---

## 2. `products`

This is the **main ‚Äúlive‚Äù table** your app queries. It merges keyword rules + enrichment when available.

```sql
CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    raw_id INT REFERENCES products_raw(raw_id) ON DELETE CASCADE,
    title TEXT,
    brand TEXT,
    price NUMERIC(12,2),
    category TEXT,          -- broad (Clothing, Beauty, Electronics, etc.)
    gender TEXT,            -- Men's / Women's / Unisex / N/A
    tags TEXT[],            -- style, features, etc.
    specs JSONB,            -- structured specs (key:value pairs)
    image_urls TEXT[],
    confidence_score NUMERIC(3,2),  -- how confident processing is
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

* **Purpose:** Finalized product record for use in app.
* **Why:** Keeps everything normalized and query-friendly.

---

## 3. `products_enriched`

This is where your **LLM output** lands. It‚Äôs separate so you can re-run, audit, or even swap models later.

```sql
CREATE TABLE products_enriched (
    enrich_id SERIAL PRIMARY KEY,
    raw_id INT REFERENCES products_raw(raw_id) ON DELETE CASCADE,
    normalized_category TEXT,
    extra_tags TEXT[],
    structured_specs JSONB,
    llm_model_used TEXT,
    enriched_at TIMESTAMP DEFAULT NOW()
);
```

* **Purpose:** Store AI-generated insights.
* **Why:** Avoid mixing machine outputs with rule outputs until you merge into `products`.

---

# üîπ Flow in Practice

1. **Scrape ‚Üí insert into `products_raw`**

   * Example: `"Nike Air Zoom Pegasus 39 Men‚Äôs Running Shoes"`

2. **Rule Engine ‚Üí insert/update `products`**

   * Brand = Nike (keyword match)
   * Category = Shoes (from breadcrumb/title)
   * Gender = Men‚Äôs (regex on title)

3. **LLM Batch ‚Üí insert into `products_enriched`**

   * LLM extracts: `extra_tags = ["running", "cushioned", "road"]`
   * `structured_specs = {"material":"mesh", "midsole":"ZoomX foam"}`

4. **Merge ‚Üí update `products` with enriched data**

   * Tags in `products` = union of keyword tags + AI tags
   * Specs = richer JSON than rules alone

---

# üîπ Why This Is Strong

* **Flexibility:** You can run without LLM at all, but add it later seamlessly.
* **Auditability:** If an LLM tags something wrong, you know which model + when.
* **Scalability:** Can re-process thousands of items in batches without touching live app data.
* **App-Friendly:** Frontend queries `products` only ‚Äî always clean and consistent.

---

üëâ Question for you: do you want me to also **map out a predefined category + tag keyword dictionary** (e.g., Clothing, Beauty, Tech, etc.) that your rules engine would use in step 2 before LLM enrichment? That way you can start building structure right now.
