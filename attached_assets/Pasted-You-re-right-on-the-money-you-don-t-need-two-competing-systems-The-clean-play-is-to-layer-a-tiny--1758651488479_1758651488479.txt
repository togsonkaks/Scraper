You’re right on the money: you don’t need two competing systems. The clean play is to layer a tiny, hi-res “augmenter” inside your existing engine that only contributes additional big images (the ones that load after click/zoom/lazy), and then let your current filtration/scoring do its job unchanged.Below is a surgical patch: a single function you can drop into your codebase and call from your current collector. It does not touch your filters, does not dedupe early, and it’s scoped to product/gallery areas so it won’t spam other sites.Drop-in: collectHiResAugment()// Hi-Res Augmenter — add-on for your existing engine
// Purpose: discover big images that appear only after click/zoom/lazy hydration
// Safe-by-default: scoped to gallery containers; no normalization/dedupe here.

async function collectHiResAugment({
  doc = document,
  observeMs = 1200,           // brief watch to catch lazy hydration / zoom swaps
  max = 40,                   // soft cap; your filter will trim further
  scopeSelectors = [
    // Common product galleries (Amazon/Shopify/generic)
    '#imageBlock', '#altImages', '[data-a-dynamic-image]',
    '.product-gallery', '.product-images', '.product-media',
    '.product-single__photo', '.flickity-viewport',
    'main figure', 'main .gallery', 'article figure'
  ]
} = {}) {
  const live = window.document || doc;
  const urls = [];
  const seen = new Set();

  const add = (u) => {
    if (!u) return;
    // No normalization here — your existing filtration/upgrade pipeline will handle it.
    try { u = new URL(u, location.href).toString(); } catch {}
    if (!/^https?:\/\//i.test(u)) return;
    if (seen.has(u)) return;
    seen.add(u);
    urls.push(u);
  };

  const pickSrcsetLargest = (ss) => {
    return (ss || "")
      .split(",")
      .map(s => s.trim())
      .map(s => {
        const [u, d] = s.split(/\s+/);
        const w = d?.endsWith("w") ? parseInt(d) : 0;
        const x = d?.endsWith("x") ? parseFloat(d) : 0;
        return { u, score: w || x * 1000 || 0 };
      })
      .filter(x => x.u)
      .sort((a,b) => b.score - a.score)[0]?.u;
  };

  // 1) Find a gallery scope; if none, fall back to doc (but we prefer scope)
  let scope = null;
  for (const sel of scopeSelectors) {
    const n = doc.querySelector(sel);
    if (n) { scope = n; break; }
  }
  if (!scope) scope = doc;

  // 2) One-shot pass inside scope
  const scanScopeOnce = () => {
    // Full-size/zoom swaps (Amazon immersive, generic zoomers)
    scope.querySelectorAll('img.fullscreen, .ivLargeImage img').forEach(img => {
      add(img.currentSrc || img.src);
    });

    // Regular imgs + lazy attrs + <picture><source>
    const LAZY = [
      'data-src','data-srcset','data-lazy','data-lazy-src','data-original',
      'data-zoom-image','data-large_image','data-hires','data-defer-src',
      'data-defer-srcset','data-flickity-lazyload'
    ];

    scope.querySelectorAll('img').forEach(img => {
      const best = img.currentSrc || pickSrcsetLargest(img.getAttribute('srcset')) || img.getAttribute('src');
      if (best) add(best);
      LAZY.forEach(a => {
        const v = img.getAttribute(a);
        if (!v) return;
        if (a.endsWith('srcset')) {
          const u = pickSrcsetLargest(v); if (u) add(u);
        } else add(v);
      });
    });

    scope.querySelectorAll('picture source[srcset]').forEach(s => {
      const u = pickSrcsetLargest(s.getAttribute('srcset')); if (u) add(u);
    });

    // Background images inside scope (hero/product cards)
    const urlRe = /url\((['"]?)(.*?)\1\)/i;
    scope.querySelectorAll('[style]').forEach(el => {
      const m = urlRe.exec(el.getAttribute('style') || ''); if (m?.[2]) add(m[2]);
    });
  };

  // 3) Amazon a-state (read from live document; sanitized clones strip <script>)
  const grabAState = () => {
    const states = live.querySelectorAll('script[type="a-state"][data-a-state],script[type="application/json"][data-a-state]');
    states.forEach(s => {
      try {
        const keyAttr = s.getAttribute('data-a-state');
        const key = keyAttr ? JSON.parse(keyAttr).key || '' : '';
        if (!/image\-block\-state|imageState|dpx\-image\-state/i.test(key)) return;
        const payload = JSON.parse(s.textContent || '{}');
        const gallery = payload?.imageGalleryData
          || payload?.imageBlock?.imageGalleryData
          || payload?.colorImages?.initial
          || [];
        (gallery || []).forEach(o => {
          ['hiRes','mainUrl','large','zoom','thumb','variant'].forEach(k => o?.[k] && add(o[k]));
        });
        const atf = payload?.ImageBlockATF || payload?.imageBlock?.ImageBlockATF;
        if (atf?.hiRes) add(atf.hiRes);
        (atf?.variant || []).forEach(add);
      } catch {}
    });
  };

  // 4) Observe briefly to catch lazy/zoom swaps (you or the site can click; we just listen)
  scanScopeOnce();
  grabAState();

  await new Promise(resolve => {
    const obs = new MutationObserver(() => {
      scanScopeOnce();
    });
    try { window.scrollBy(0, 1); window.scrollBy(0, -1); } catch {}
    obs.observe(scope, { subtree: true, childList: true, attributes: true });
    setTimeout(() => { obs.disconnect(); resolve(); }, observeMs);
  });

  // 5) Soft cap; your existing filtration pipeline will finalize ranking
  return urls.slice(0, max);
}
How to integrate (without touching your filtration)You’ll have a function today that gathers raw candidates, then passes them through your existing “hybrid” filter/scorer. Keep that exactly as is and just add this one line:// BEFORE (simplified)
const raw = await gatherImagesBySelector(document);  // your current engine
const final = await hybridUniqueImages(raw);         // your existing filter/scorer

// AFTER (augmented)
const raw = await gatherImagesBySelector(document);                 // your current engine
const hiResAugment = await collectHiResAugment({ doc: document });  // NEW: adds big/lazy/zoom images
const mergedRaw = raw.concat(hiResAugment);                         // no dedupe/normalize here
const final = await hybridUniqueImages(mergedRaw);                  // your existing filter/scorer
No edits to your current filtration/scoring/normalization.The augmenter only adds candidates, especially the ones that appear after click/zoom.It’s scoped to the product/gallery containers first; if none found, it harmlessly falls back to the whole doc.It peeks at Amazon’s a-state in the live DOM so Electron’s sanitized clone doesn’t break it.Why this won’t break sites that already workIt doesn’t change your pipeline’s decisions; it just offers more (and bigger) candidates for your existing filter to evaluate.It doesn’t click or mutate anything; it only observes briefly (so carousels/zoomers that hydrate after user click will still be captured).If a site has no lazy/zoom nuance, the augment returns little or nothing and gets out of the way.
